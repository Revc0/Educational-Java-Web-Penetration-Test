User-agent: *
Disallow: /admin/
Disallow: /backup.sql  # Pointing to the backup file
Disallow: /backup/
Disallow: /test/
Disallow: /dev/
Disallow: /tmp/hidden-files/



With this configuration, while respectful web crawlers would avoid indexing or accessing the paths listed in the robots.txt,
a curious individual (or an attacker) might be inclined to check out the disallowed paths, particularly if they notice
a .sql file mentioned, which could indicate a database dump or backup.

Remember, a robots.txt file is publicly accessible and it is considered bad practice to list sensitive or critical files in it.
Instead, secure methods should be used to protect sensitive files, and they should not be accessible from the web root.
This exercise is purely for educational purposes to demonstrate potential misconfigurations.